use std::vec;

use non_terminal::NonTerminal;
use parse_error::{ParseError, ParseError2};
use token_stack::TokenStack;

use crate::lexer::{
    reserved_word::ReservedWord,
    token::{Token, TokenBase},
};

use self::{
    ast::Ast,
    invalid_syntax::{ExpectedActuallyTokenPair, InvalidSyntax, InvalidSyntaxType},
};

pub mod ast;
pub mod ast_type;
mod invalid_syntax;
pub(crate) mod non_terminal;
mod parse_error;
pub mod token_stack;

/// abbreviation for TokenBase::Reserved
// this section is auto-generated by below codes.
// fn main() {
//      let reserveds = [
//          ...all of reserved words.
//      ];
//      let mut st = String::new();
//      st.push_str("macro_rules! TBR {\n");
//      for item in reserveds.iter() {
//          st.push_str(&format!(r#"("{}") => {{crate::lexer::token::TokenBase::Reserved(crate::lexer::reserved_word::ReservedWord::{:?})}};"#, item.to_string(), item));
//          st.push('\n');
//      }
//      st.push('}');
//      println!("{}", st);
// }

macro_rules! TBR {
    ("=") => {
        crate::lexer::token::TokenBase::Reserved(crate::lexer::reserved_word::ReservedWord::Assign)
    };
    ("(") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::LeftParenthesis,
        )
    };
    (")") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::RightParenthesis,
        )
    };
    ("{") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::LeftCurly,
        )
    };
    ("}") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::RightCurly,
        )
    };
    ("[") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::LeftSquareBracket,
        )
    };
    ("]") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::RightSquareBracket,
        )
    };
    (".") => {
        crate::lexer::token::TokenBase::Reserved(crate::lexer::reserved_word::ReservedWord::Dot)
    };
    (",") => {
        crate::lexer::token::TokenBase::Reserved(crate::lexer::reserved_word::ReservedWord::Comma)
    };
    (";") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::SemiColon,
        )
    };
    ("+") => {
        crate::lexer::token::TokenBase::Reserved(crate::lexer::reserved_word::ReservedWord::Add)
    };
    ("*") => {
        crate::lexer::token::TokenBase::Reserved(crate::lexer::reserved_word::ReservedWord::Mult)
    };
    ("/") => {
        crate::lexer::token::TokenBase::Reserved(crate::lexer::reserved_word::ReservedWord::Div)
    };
    ("-") => {
        crate::lexer::token::TokenBase::Reserved(crate::lexer::reserved_word::ReservedWord::Sub)
    };
    ("%") => {
        crate::lexer::token::TokenBase::Reserved(crate::lexer::reserved_word::ReservedWord::Mod)
    };
    ("<") => {
        crate::lexer::token::TokenBase::Reserved(crate::lexer::reserved_word::ReservedWord::Less)
    };
    (">") => {
        crate::lexer::token::TokenBase::Reserved(crate::lexer::reserved_word::ReservedWord::Greater)
    };
    ("&") => {
        crate::lexer::token::TokenBase::Reserved(crate::lexer::reserved_word::ReservedWord::And)
    };
    ("|") => {
        crate::lexer::token::TokenBase::Reserved(crate::lexer::reserved_word::ReservedWord::Or)
    };
    ("^") => {
        crate::lexer::token::TokenBase::Reserved(crate::lexer::reserved_word::ReservedWord::Xor)
    };
    ("~") => {
        crate::lexer::token::TokenBase::Reserved(crate::lexer::reserved_word::ReservedWord::Not)
    };
    ("!") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::LogicalNot,
        )
    };
    ("=>") => {
        crate::lexer::token::TokenBase::Reserved(crate::lexer::reserved_word::ReservedWord::Arrow)
    };
    ("const") => {
        crate::lexer::token::TokenBase::Reserved(crate::lexer::reserved_word::ReservedWord::Const)
    };
    ("let") => {
        crate::lexer::token::TokenBase::Reserved(crate::lexer::reserved_word::ReservedWord::Let)
    };
    ("import") => {
        crate::lexer::token::TokenBase::Reserved(crate::lexer::reserved_word::ReservedWord::Import)
    };
    ("export") => {
        crate::lexer::token::TokenBase::Reserved(crate::lexer::reserved_word::ReservedWord::Export)
    };
    ("default") => {
        crate::lexer::token::TokenBase::Reserved(crate::lexer::reserved_word::ReservedWord::Default)
    };
    ("from") => {
        crate::lexer::token::TokenBase::Reserved(crate::lexer::reserved_word::ReservedWord::From)
    };
    ("false") => {
        crate::lexer::token::TokenBase::Reserved(crate::lexer::reserved_word::ReservedWord::False)
    };
    ("true") => {
        crate::lexer::token::TokenBase::Reserved(crate::lexer::reserved_word::ReservedWord::True)
    };
    ("<<") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::LeftShift,
        )
    };
    (">>") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::RightShift,
        )
    };
    (">>>") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::UnsignedRightShift,
        )
    };
    ("<=") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::LessOrEq,
        )
    };
    (">=") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::GreaterOrEq,
        )
    };
    ("==") => {
        crate::lexer::token::TokenBase::Reserved(crate::lexer::reserved_word::ReservedWord::Equal)
    };
    ("!=") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::NotEqual,
        )
    };
    ("**") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::Exponential,
        )
    };
    ("&&") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::LogicalAnd,
        )
    };
    ("||") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::LogicalOr,
        )
    };
    ("+=") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::AdditiveAssign,
        )
    };
    ("-=") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::SubtractiveAssign,
        )
    };
    ("*=") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::MultiplicativeAssign,
        )
    };
    ("/=") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::DivisiveAssign,
        )
    };
    ("%=") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::ModuloAssign,
        )
    };
    ("<<=") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::LeftShiftAssign,
        )
    };
    (">>=") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::RightShiftAssign,
        )
    };
    (">>>=") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::UnsignedRightShiftAssign,
        )
    };
    ("&=") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::AndAssign,
        )
    };
    ("^=") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::XorAssign,
        )
    };
    ("|=") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::OrAssign,
        )
    };
    ("**=") => {
        crate::lexer::token::TokenBase::Reserved(
            crate::lexer::reserved_word::ReservedWord::ExponentialAssign,
        )
    };
}

trait InvalidSyntaxResultHandler {
    fn handle_scan(self, perror: &mut ParseError2);
    fn handle_consume(self, parser: &mut Parser);
}

impl InvalidSyntaxResultHandler for Result<(), InvalidSyntax> {
    fn handle_scan(self, perror: &mut ParseError2) {
        self.unwrap_or_else(|e| perror.add_error(e));
    }

    fn handle_consume(self, parser: &mut Parser) {
        self.unwrap_or_else(|e| {
            parser.parse_error.add_error(e);
            parser.tokens.next();
        });
    }
}

mod expression_parser;
mod import_parser;

pub struct Parser<'a> {
    pub tokens: &'a mut TokenStack<'a>,
    pub ast: Ast,
    parse_error: ParseError2,
}

impl<'a> Parser<'a> {
    pub fn new(tokens: &'a mut TokenStack<'a>) -> Parser<'a> {
        Parser {
            tokens,
            ast: Ast::new_node_with_leaves(NonTerminal::TranslationUnit, Vec::new()),
            parse_error: ParseError2::new(),
        }
    }

    pub fn parse(&mut self) -> Result<(), &ParseError2> {
        self.parse_translation_unit().unwrap();
        if self.parse_error.has_error() {
            Err(&self.parse_error)
        } else {
            Ok(())
        }
    }

    // TranslationUnit = { ImportDeclaration } , { { Attribute } , ExportableConstDeclaration };
    fn parse_translation_unit(&mut self) -> Result<(), ParseError> {
        while let Some(TokenBase::Reserved(ReservedWord::Import)) = self.tokens.look_ahead(1) {
            let import = self.parse_import_declaration()?;
            self.ast.add_child(import);
        }
        while self.tokens.has_next() {
            let c = match self.tokens.look_ahead(1) {
                Some(TokenBase::Reserved(ReservedWord::LeftSquareBracket)) => {
                    while Some(TokenBase::Reserved(ReservedWord::LeftSquareBracket))
                        == self.tokens.look_ahead(1)
                    {
                        let child = self.parse_attribute()?;
                        self.ast.add_child(child);
                    }
                    if Some(TokenBase::Reserved(ReservedWord::Const)) == self.tokens.look_ahead(1) {
                        self.parse_exportable_const_declaration()?
                    } else {
                        let target_token = self.tokens.nth(1);
                        self.handle_expected_actually_error(
                            target_token.clone(),
                            vec![TokenBase::Reserved(ReservedWord::Const)],
                            self.tokens.peek_token().unwrap(),
                        );
                        Ast::new_leaf(target_token.unwrap())
                    }
                }
                Some(TokenBase::Reserved(ReservedWord::Const))
                | Some(TokenBase::Reserved(ReservedWord::Export)) => {
                    self.parse_exportable_const_declaration()?
                }
                Some(_) | None => {
                    let target = self.tokens.nth(1);
                    self.handle_expected_actually_error(
                        self.tokens.nth(1),
                        vec![
                            TokenBase::Reserved(ReservedWord::LeftSquareBracket),
                            TokenBase::Reserved(ReservedWord::Const),
                            TokenBase::Reserved(ReservedWord::Export),
                        ],
                        self.tokens.peek_token().unwrap(),
                    );
                    Ast::new_leaf(target.unwrap())
                }
            };
            self.ast.add_child(c);
        }

        Ok(())
    }

    #[allow(clippy::unnecessary_wraps)]
    fn parse_attribute(&mut self) -> Result<Ast, ParseError> {
        self.tokens.next();
        let ast2 = match self.tokens.look_ahead(1) {
            Some(TokenBase::Identifier(_)) => {
                let attr_token = self.tokens.next_token().unwrap();
                self.tokens
                    .consume_reserved(ReservedWord::RightSquareBracket)
                    .handle_consume(self);
                Ast::new_node_with_leaves(NonTerminal::Attribute, vec![Ast::new_leaf(attr_token)])
            }
            None | Some(_) => {
                let target_token = self.tokens.nth(1);
                self.handle_expected_actually_error(
                    target_token.clone(),
                    vec![TokenBase::default_identifier()],
                    self.tokens.peek_token().unwrap(),
                );
                Ast::new_leaf(target_token.unwrap())
            }
        };

        Ok(ast2)
    }

    // ExportableConstDeclaration = [ "export" , ["default"] ] , ConstDeclaration;
    fn parse_exportable_const_declaration(&mut self) -> Result<Ast, ParseError> {
        let mut ast = Vec::new();

        match self.tokens.look_ahead(1) {
            Some(TokenBase::Reserved(ReservedWord::Export)) => {
                ast.push(Ast::new_leaf(self.tokens.next_token().unwrap()));
                match self.tokens.look_ahead(1) {
                    Some(TokenBase::Reserved(ReservedWord::Default)) => {
                        ast.push(Ast::new_leaf(self.tokens.next_token().unwrap()));
                        self.tokens
                            .scan_reserved(ReservedWord::Const)
                            .handle_scan(&mut self.parse_error);
                        ast.push(self.parse_const_declaration()?);
                    }
                    Some(TokenBase::Reserved(ReservedWord::Const)) => {
                        ast.push(self.parse_const_declaration()?);
                    }
                    Some(_) | None => {
                        self.handle_expected_actually_error(
                            self.tokens.nth(1),
                            vec![
                                TokenBase::Reserved(ReservedWord::Default),
                                TokenBase::Reserved(ReservedWord::Const),
                            ],
                            self.tokens.peek_token().unwrap(),
                        );
                    }
                }
            }
            Some(TokenBase::Reserved(ReservedWord::Const)) => {
                ast.push(self.parse_const_declaration()?);
            }
            Some(_) | None => {
                let target_token = self.tokens.nth(1);
                self.handle_expected_actually_error(
                    target_token,
                    vec![
                        TokenBase::Reserved(ReservedWord::Const),
                        TokenBase::Reserved(ReservedWord::Export),
                    ],
                    self.tokens.peek_token().unwrap(),
                );
            }
        }
        Ok(Ast::new_node_with_leaves(
            NonTerminal::ExportableConstDeclaration,
            ast,
        ))
    }

    fn parse_const_declaration(&mut self) -> Result<Ast, ParseError> {
        self.tokens.next();
        Ok(Ast::new_node_with_leaves(
            NonTerminal::ConstDeclaration,
            vec![self.parse_declaration_body()?],
        ))
    }

    fn parse_let_declaration(&mut self) -> Result<Ast, ParseError> {
        self.tokens.next();
        Ok(Ast::new_node_with_leaves(
            NonTerminal::LetDeclaration,
            vec![self.parse_declaration_body()?],
        ))
    }

    fn parse_declaration_body(&mut self) -> Result<Ast, ParseError> {
        let ident_ast = if let Some(TokenBase::Identifier(_)) = self.tokens.look_ahead(1) {
            Ast::new_leaf(self.tokens.next_token().unwrap())
        } else {
            let target_token = self.tokens.nth(1);
            self.handle_expected_actually_error(
                target_token.clone(),
                vec![TokenBase::default_identifier()],
                self.tokens.peek_token().unwrap(),
            );
            Ast::new_leaf(target_token.unwrap())
        };
        self.tokens
            .consume_reserved(ReservedWord::Assign)
            .handle_consume(self);
        let expr_ast = self.parse_expression()?;
        if let Some(TokenBase::Reserved(ReservedWord::SemiColon)) = self.tokens.look_ahead(1) {
            self.tokens.next();
        } else {
            self.handle_expected_actually_error(
                self.tokens.nth(1),
                vec![TokenBase::Reserved(ReservedWord::SemiColon)],
                self.tokens.peek_token().unwrap(),
            );
        }
        Ok(Ast::new_node_with_leaves(
            NonTerminal::DeclarationBody,
            vec![ident_ast, expr_ast],
        ))
    }

    fn parse_args(&mut self) -> Result<Ast, ParseError> {
        self.tokens
            .consume_reserved(ReservedWord::LeftParenthesis)
            .handle_consume(self);
        let mut callers = Vec::new();

        if self.tokens.look_ahead(1) == Some(TokenBase::Reserved(ReservedWord::RightParenthesis)) {
            self.tokens.next();
            Ok(Ast::new_node_with_leaves(NonTerminal::Args, Vec::new()))
        } else {
            loop {
                match self.tokens.look_ahead(1) {
                    Some(TokenBase::String(_))
                    | Some(TokenBase::Number(_))
                    | Some(TokenBase::Identifier(_))
                    | Some(TokenBase::Reserved(ReservedWord::LeftParenthesis)) => {
                        callers.push(self.parse_expression()?);
                        match self.tokens.look_ahead(1) {
                            Some(TokenBase::Reserved(ReservedWord::Comma)) => {
                                self.tokens.next();
                            }
                            Some(TokenBase::Reserved(ReservedWord::RightParenthesis)) => {
                                self.tokens.next();
                                break;
                            }
                            Some(_) | None => {
                                self.handle_expected_actually_error(
                                    self.tokens.nth(1),
                                    vec![
                                        TokenBase::Reserved(ReservedWord::Comma),
                                        TokenBase::Reserved(ReservedWord::RightParenthesis),
                                    ],
                                    self.tokens.peek_token().unwrap(),
                                );
                            }
                        }
                    }
                    Some(_) | None => {
                        self.handle_expected_actually_error(
                            self.tokens.nth(1),
                            vec![
                                TokenBase::Reserved(ReservedWord::Comma),
                                TokenBase::Reserved(ReservedWord::RightParenthesis),
                            ],
                            self.tokens.peek_token().unwrap(),
                        );
                        break;
                    }
                }
            }
            Ok(Ast::new_node_with_leaves(NonTerminal::Args, callers))
        }
    }

    fn handle_expected_actually_error(
        &mut self,
        target_token: Option<Token>,
        expected_tokens: Vec<TokenBase>,
        before_token: Token,
    ) {
        match target_token {
            Some(tk) => {
                self.parse_error.add_error(InvalidSyntax::new(
                    tk.get_token_position(),
                    InvalidSyntaxType::ExpectedNext(ExpectedActuallyTokenPair(expected_tokens, tk)),
                ));
                self.tokens.next();
            }
            None => {
                self.handle_unexpected_eof_error(before_token);
            }
        }
    }

    fn handle_unexpected_eof_error(&mut self, before_token: Token) {
        self.parse_error.add_error(InvalidSyntax::new(
            before_token.get_token_position(),
            InvalidSyntaxType::UnexpectedEof,
        ));
    }
}

#[cfg(test)]
mod tests {

    #[test]
    fn test_name() {}
}
